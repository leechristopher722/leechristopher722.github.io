[{"content":"","date":null,"permalink":"/tags/algorithms/","section":"Tags","summary":"","title":"Algorithms"},{"content":" My Learning Journey ","date":null,"permalink":"/blog/","section":"Blog","summary":" My Learning Journey ","title":"Blog"},{"content":" Designing and Developing Tomorrow\u0026rsquo;s Solutions Welcome to my portfolio! Discover my projects, blog, and experiences, and feel free to reach out. I\u0026rsquo;m always excited to collaborate on new challenges and opportunities!\n","date":null,"permalink":"/","section":"Christopher Lee","summary":"Designing and Developing Tomorrow\u0026rsquo;s Solutions Welcome to my portfolio!","title":"Christopher Lee"},{"content":"","date":null,"permalink":"/tags/data-structures/","section":"Tags","summary":"","title":"Data Structures"},{"content":"","date":null,"permalink":"/tags/interview-prep/","section":"Tags","summary":"","title":"Interview Prep"},{"content":"","date":null,"permalink":"/tags/leetcode/","section":"Tags","summary":"","title":"LeetCode"},{"content":"Let\u0026rsquo;s address the elephant in the room: LeetCode isn\u0026rsquo;t about becoming a better programmer. It\u0026rsquo;s about passing technical interviews. The skills overlap, but they\u0026rsquo;re not identical. Accepting this makes the process less frustrating.\nThe Pattern Recognition Game Technical interviews aren\u0026rsquo;t testing if you can invent algorithms‚Äîthey\u0026rsquo;re testing if you can recognize and apply patterns. Here are the patterns that cover 90% of problems:\n1. Two Pointers When to use: Arrays, strings, linked lists with comparison or search operations\nClassic Problems:\nTwo Sum (sorted array) Container With Most Water Trapping Rain Water Remove Duplicates The Template:\ndef two_pointer_template(arr): left, right = 0, len(arr) - 1 while left \u0026lt; right: # Process current state if condition_met: return result elif need_smaller: left += 1 else: right -= 1 return not_found Key Insight: Often converts O(n¬≤) brute force to O(n) or O(n log n) with sorting.\n2. Sliding Window When to use: Subarray/substring problems with contiguous elements\nFixed Window:\ndef fixed_window(arr, k): window_sum = sum(arr[:k]) max_sum = window_sum for i in range(k, len(arr)): window_sum = window_sum - arr[i-k] + arr[i] max_sum = max(max_sum, window_sum) return max_sum Variable Window:\ndef variable_window(s, condition): left = 0 result = 0 window = {} # or whatever you\u0026#39;re tracking for right in range(len(s)): # Expand window add_to_window(s[right]) # Contract window while invalid while not valid(window): remove_from_window(s[left]) left += 1 # Update result result = max(result, right - left + 1) return result 3. Fast \u0026amp; Slow Pointers When to use: Cycle detection, middle element, linked list problems\nClassic Applications:\ndef has_cycle(head): slow = fast = head while fast and fast.next: slow = slow.next fast = fast.next.next if slow == fast: return True return False def find_middle(head): slow = fast = head while fast and fast.next: slow = slow.next fast = fast.next.next return slow 4. Tree Traversal Patterns The Three Questions for Any Tree Problem:\nCan I solve it by visiting nodes in a specific order? (Traversal) Can I solve it by getting info from children first? (Post-order) Can I solve it top-down passing info to children? (Pre-order) The Universal Template:\ndef tree_recursion(root, additional_params): # Base case if not root: return base_value # Pre-order processing # Recurse left_result = tree_recursion(root.left, updated_params) right_result = tree_recursion(root.right, updated_params) # Post-order processing return combined_result 5. BFS / Level-Order When to use: Shortest path, level-by-level processing, trees/graphs\nThe Template That Handles Everything:\nfrom collections import deque def bfs_template(start): queue = deque([start]) visited = {start} level = 0 while queue: # Process entire level at once level_size = len(queue) for _ in range(level_size): node = queue.popleft() # Process node if is_target(node): return level # Add neighbors for neighbor in get_neighbors(node): if neighbor not in visited: visited.add(neighbor) queue.append(neighbor) level += 1 return -1 6. DFS / Backtracking When to use: All paths, all combinations, all permutations, puzzles\nThe Backtracking Template:\ndef backtrack(choices, path, result): # Base case if is_complete(path): result.append(path[:]) # Important: copy the path return for i, choice in enumerate(choices): # Skip invalid choices if not is_valid(choice, path): continue # Make choice path.append(choice) # Recurse with remaining choices backtrack(choices[i+1:], path, result) # or choices[:i] + choices[i+1:] for perms # Undo choice path.pop() 7. Dynamic Programming The Two Questions:\nCan I break this into overlapping subproblems? Can I define the solution recursively? Top-Down (Memoization):\ndef dp_recursive(n, memo={}): # Base case if n \u0026lt;= 1: return base_value # Check memo if n in memo: return memo[n] # Recurrence relation memo[n] = dp_recursive(n-1) + dp_recursive(n-2) # or whatever return memo[n] Bottom-Up (Tabulation):\ndef dp_iterative(n): if n \u0026lt;= 1: return base_value dp = [0] * (n + 1) dp[0] = base1 dp[1] = base2 for i in range(2, n + 1): dp[i] = dp[i-1] + dp[i-2] # recurrence relation return dp[n] Space Optimization (when you only need previous few values):\ndef dp_optimized(n): if n \u0026lt;= 1: return base_value prev2, prev1 = base1, base2 for i in range(2, n + 1): current = prev1 + prev2 prev2, prev1 = prev1, current return prev1 8. Binary Search Beyond Sorted Arrays:\ndef binary_search_template(search_space): left, right = min_value, max_value while left \u0026lt; right: mid = left + (right - left) // 2 if condition(mid): right = mid # or left = mid + 1 else: left = mid + 1 # or right = mid return left The Trick: If you can frame the problem as \u0026ldquo;find the minimum/maximum value where condition is true,\u0026rdquo; use binary search.\nData Structure Instant Decisions When to Use What Array/List:\nNeed index access Fixed size or know size upfront Cache-friendly operations HashMap/Dict:\nO(1) lookup needed Counting occurrences Mapping relationships Finding pairs/complements Set:\nUniqueness matters O(1) membership testing Finding duplicates Stack:\nMatching brackets/parentheses Nearest greater/smaller element Evaluate expressions Backtracking (implicit with recursion) Queue/Deque:\nBFS Level-order traversal Sliding window maximum Process in order Heap/Priority Queue:\nK-th largest/smallest Top K frequent Merge K sorted lists Dijkstra\u0026rsquo;s algorithm Trie:\nPrefix matching Autocomplete Word search in grid The Time Complexity Reality Check Stop memorizing‚Äîunderstand the patterns:\nO(1): Direct access, math operations O(log n): Binary search, balanced tree operations O(n): Single pass through data O(n log n): Sorting, divide \u0026amp; conquer O(n¬≤): Nested loops, comparing all pairs O(2‚Åø): Subsets, recursive branching O(n!): Permutations\nThe Interview Optimization Path:\nBrute force (usually O(n¬≤) or worse) Can I sort first? (O(n log n)) Can I use a hash map? (O(n) space for O(1) lookup) Can I use two pointers? (O(1) space) Is there a greedy approach? Do I need DP? Common Gotchas and How to Avoid Them Python Specific Traps # Gotcha 1: Mutable default arguments def bad(arr=[]): # DON\u0026#39;T arr.append(1) return arr def good(arr=None): # DO if arr is None: arr = [] arr.append(1) return arr # Gotcha 2: Integer division result = a // b # Floor division (what you usually want) result = int(a / b) # Truncation toward zero (different for negatives!) # Gotcha 3: List reference vs copy path.append(current_path) # BAD - stores reference path.append(current_path[:]) # GOOD - stores copy # Gotcha 4: String immutability s[0] = \u0026#39;a\u0026#39; # ERROR - strings are immutable s = list(s) # Convert to list first s[0] = \u0026#39;a\u0026#39; s = \u0026#39;\u0026#39;.join(s) The Edge Cases Checklist Always test:\nEmpty input ([], \u0026ldquo;\u0026rdquo;, None) Single element All same elements Negative numbers Overflow (for int problems) Cycles (for graphs/linked lists) Disconnected components (for graphs) The Interview Strategy Before You Code Clarify constraints: Input size? Value ranges? Can I modify input? Work through examples: Start small, find patterns Discuss approach: Brute force ‚Üí optimized Analyze complexity: Time and space Then code: Only after agreement on approach While Coding Think out loud: Silence is your enemy Write clean code: Variable names matter Handle edges: But don\u0026rsquo;t obsess initially Test as you go: Don\u0026rsquo;t wait until the end The 45-Minute Timeline 0-5 min: Understand problem 5-10 min: Work through examples 10-15 min: Discuss approach 15-35 min: Code solution 35-40 min: Test and debug 40-45 min: Discuss optimizations The Practice Strategy That Works Week 1-2: Fundamentals 5 array problems 5 string problems 5 linked list problems Focus on brute force, don\u0026rsquo;t optimize yet Week 3-4: Core Patterns 10 two-pointer problems 10 sliding window problems Learn the templates, apply mechanically Week 5-6: Trees and Graphs 10 tree problems 10 graph problems (BFS/DFS) Master the traversal templates Week 7-8: Dynamic Programming Start with classic problems (Fibonacci, coin change) Focus on identifying recurrence relations Don\u0026rsquo;t memorize solutions Week 9-12: Mixed Practice 3-5 problems daily Time yourself Focus on mediums One hard per week The Spaced Repetition Secret Solve problem Try again in 3 days Try again in 1 week Try again in 1 month If you can\u0026rsquo;t solve it from scratch each time, you haven\u0026rsquo;t learned the pattern.\nWhen You\u0026rsquo;re Stuck The Unstuck Protocol Re-read the problem: Did I miss constraints? Try more examples: Look for patterns Simplify: Solve for n=2, then generalize Consider all patterns: Go through the list above Check discussions after 30 minutes: Learn, don\u0026rsquo;t just copy The Learning Mindset It\u0026rsquo;s not about intelligence: It\u0026rsquo;s about pattern recognition Everyone struggles initially: Including FAANG engineers Progress isn\u0026rsquo;t linear: Breakthrough moments happen suddenly Quality over quantity: Understanding 100 problems beats rushing through 500 The Meta-Game What Interviewers Actually Care About Problem-solving approach: Not just the solution Communication: Can you explain your thinking? Code quality: Is it readable and maintainable? Debugging skills: How do you handle bugs? Optimization: Can you improve your solution? Red Flags to Avoid Jumping to code immediately Not asking clarifying questions Writing unreadable code Giving up too quickly Not testing your solution Being arrogant or dismissive Conclusion: The Long Game LeetCode is a game with specific rules. Learn the rules, practice the patterns, and you\u0026rsquo;ll improve. But remember:\nIt\u0026rsquo;s a learnable skill: Not a measure of your worth as a developer Consistency beats intensity: 30 minutes daily beats 5-hour weekend sessions Understanding beats memorization: Learn patterns, not solutions It\u0026rsquo;s temporary: Once you get the job, you\u0026rsquo;ll rarely use these specific skills The developers who succeed at LeetCode aren\u0026rsquo;t necessarily the smartest‚Äîthey\u0026rsquo;re the ones who figured out it\u0026rsquo;s just pattern matching with extra steps.\n","date":"29 October 2025","permalink":"/blog/leetcode-guide/","section":"Blog","summary":"Let\u0026rsquo;s address the elephant in the room: LeetCode isn\u0026rsquo;t about becoming a better programmer.","title":"LeetCode Guide"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/tags/blockchain/","section":"Tags","summary":"","title":"Blockchain"},{"content":"Blockchain has been called everything from \u0026ldquo;the most important invention since the internet\u0026rdquo; to \u0026ldquo;a solution looking for a problem.\u0026rdquo; After diving deep into the technology, implementing smart contracts, and watching countless blockchain projects rise and fall, here\u0026rsquo;s my take on what blockchain actually is, what it\u0026rsquo;s good for, and where it\u0026rsquo;s probably overhyped.\nWhat Blockchain Actually Is Strip away the buzzwords and blockchain is fundamentally a distributed database with three key properties:\nImmutable: Once data is written, it can\u0026rsquo;t be changed or deleted Distributed: Multiple copies exist across different nodes Consensus-driven: All nodes agree on the current state\nThink of it as a ledger that everyone has a copy of, where new entries can only be added if the majority agrees, and old entries can never be erased. That\u0026rsquo;s it. That\u0026rsquo;s blockchain.\nThe Technical Foundation At its core, blockchain combines several existing technologies:\nCryptographic Hashing\nEach block contains a hash of the previous block, creating a chain. Change anything in a previous block, and all subsequent hashes break, making tampering immediately obvious.\nBlock 1: [Data] + [Hash: 3a42c5] Block 2: [Data] + [Previous: 3a42c5] + [Hash: 7f8b12] Block 3: [Data] + [Previous: 7f8b12] + [Hash: 9e4d33] Distributed Consensus\nNodes vote on which transactions to include. Different blockchains use different mechanisms:\nProof of Work (Bitcoin): Solve computational puzzles Proof of Stake (Ethereum 2.0): Stake currency as collateral Proof of Authority (Private chains): Designated validators Peer-to-Peer Networking\nNo central server. Nodes communicate directly, sharing transactions and blocks across the network.\nWhere Blockchain Makes Sense After seeing hundreds of \u0026ldquo;blockchain for X\u0026rdquo; pitches, patterns emerge about where this technology actually adds value:\n1. Trust-Minimized Finance Cross-border Payments\nTraditional international transfers take days and cost 3-7% in fees. Blockchain can do it in hours for pennies. This isn\u0026rsquo;t theoretical‚Äîit\u0026rsquo;s happening now with stablecoins moving billions daily.\nDecentralized Finance (DeFi)\nLending, borrowing, and trading without banks. Smart contracts automate what traditionally required intermediaries. The catch? Complexity and risk are shifted to users.\nDigital Asset Ownership\nNFTs proved digital ownership is possible (even if most implementations were questionable). The technology has legitimate uses for tickets, certificates, and digital rights management.\n2. Multi-Party Coordination Supply Chain Tracking\nWhen multiple companies need to track goods without trusting a single party\u0026rsquo;s database, blockchain provides a shared source of truth. Walmart uses it for food safety tracking.\nInterbank Settlement\nBanks don\u0026rsquo;t trust each other but need to coordinate. Blockchain can replace clearing houses for certain transactions. JP Morgan\u0026rsquo;s JPM Coin processes $1 billion daily.\nRegulatory Compliance\nImmutable audit trails that regulators can access without companies controlling the data. Useful for industries with heavy oversight.\n3. Digital Identity and Credentials Self-Sovereign Identity\nUsers control their identity data rather than platforms. Share only what\u0026rsquo;s necessary, revoke access anytime.\nAcademic Credentials\nUniversities issue tamper-proof digital diplomas. MIT has been doing this since 2017.\nMedical Records\nPatient-controlled health records that work across providers. Estonia\u0026rsquo;s healthcare system runs on blockchain.\nWhere Blockchain Doesn\u0026rsquo;t Make Sense The \u0026ldquo;blockchain everything\u0026rdquo; movement ignored fundamental limitations:\nWhen You Need Performance Blockchains are slow by design. Bitcoin processes 7 transactions/second. Ethereum manages 30. Visa handles 65,000. If you need speed, blockchain is the wrong tool.\nWhen You Need Privacy Public blockchains are transparent by design. Everyone can see every transaction. \u0026ldquo;Private blockchains\u0026rdquo; lose many of blockchain\u0026rsquo;s benefits‚Äîat that point, use a regular database.\nWhen You Can Trust a Central Authority If all parties trust a single entity, blockchain adds complexity without benefit. Your company\u0026rsquo;s internal database doesn\u0026rsquo;t need blockchain.\nWhen You Need to Delete Data GDPR\u0026rsquo;s \u0026ldquo;right to be forgotten\u0026rdquo; is incompatible with blockchain\u0026rsquo;s immutability. This killed many European blockchain projects.\nThe Real Potential: Programmable Money The breakthrough isn\u0026rsquo;t just transferring value‚Äîit\u0026rsquo;s programming it. Smart contracts enable:\nConditional Payments\n\u0026ldquo;Pay contractor when these three people confirm work is complete\u0026rdquo;\nAutomated Escrow\n\u0026ldquo;Hold payment until delivery is confirmed by GPS coordinates\u0026rdquo;\nDecentralized Governance\n\u0026ldquo;Execute decisions when 60% of token holders vote yes\u0026rdquo;\nComposable Finance\n\u0026ldquo;If ETH price \u0026gt; $2000, borrow DAI, swap for USDC, lend on Compound\u0026rdquo;\nThis programmability is genuinely new. We\u0026rsquo;ve never had money that can execute complex logic autonomously.\nEmerging Applications Worth Watching Decentralized Science (DeSci) Researchers fund projects through DAOs, publish on-chain, and share ownership of discoveries. It could reform academic publishing and research funding.\nRegenerative Finance (ReFi) Carbon credits, biodiversity tokens, and other environmental assets traded transparently. Makes green initiatives financially sustainable.\nDecentralized Physical Infrastructure (DePIN) Helium for wireless networks, Filecoin for storage, Render for GPU compute. Communities building infrastructure, owned by participants.\nGaming and Virtual Economies True ownership of in-game assets that work across games. Players as stakeholders, not just consumers.\nThe Technical Challenges Before blockchain revolutionizes everything, several problems need solving:\nThe Scalability Trilemma\nPick two: Decentralization, Security, Scalability. Every blockchain makes tradeoffs.\nEnergy Consumption\nProof of Work uses enormous energy. Ethereum\u0026rsquo;s switch to Proof of Stake reduced consumption by 99.95%, but not all chains can follow.\nInteroperability\nHundreds of blockchains that don\u0026rsquo;t talk to each other. Bridge hacks have lost billions. Standards are emerging slowly.\nUser Experience\nLost your private key? Money\u0026rsquo;s gone forever. Sent to wrong address? Irreversible. Mass adoption needs better UX.\nThe Realistic Future Blockchain won\u0026rsquo;t replace everything, but it will transform specific sectors:\nFinancial Infrastructure\nNot replacing banks, but forcing them to compete. Central Bank Digital Currencies (CBDCs) will use blockchain tech.\nDigital Ownership\nAs we spend more time in digital spaces, owning digital assets becomes crucial. Blockchain provides the infrastructure.\nAutomated Compliance\nRegulations encoded in smart contracts. Compliance by default, not by audit.\nCoordination Networks\nDAOs and similar structures for managing shared resources, from open source projects to investment clubs.\nThe Bottom Line Blockchain is a powerful tool for specific problems: removing intermediaries, creating trust between untrusting parties, and enabling programmable money. It\u0026rsquo;s not a universal solution.\nThe projects succeeding today solve real problems where blockchain\u0026rsquo;s properties‚Äîimmutability, decentralization, transparency‚Äîprovide genuine value. They don\u0026rsquo;t use blockchain because it\u0026rsquo;s trendy; they use it because it\u0026rsquo;s the right tool.\nWe\u0026rsquo;re past the \u0026ldquo;blockchain will fix everything\u0026rdquo; phase and entering the \u0026ldquo;blockchain will fix these specific things really well\u0026rdquo; phase. That\u0026rsquo;s not as exciting, but it\u0026rsquo;s far more valuable.\nThe revolution isn\u0026rsquo;t replacing everything with blockchain. It\u0026rsquo;s having blockchain as an option when we need trust without trusted parties, coordination without coordinators, and money that can think for itself.\n","date":"20 October 2025","permalink":"/blog/blockchain/","section":"Blog","summary":"Blockchain has been called everything from \u0026ldquo;the most important invention since the internet\u0026rdquo; to \u0026ldquo;a solution looking for a problem.","title":"Blockchain: Beyond the Hype"},{"content":"","date":null,"permalink":"/tags/cryptography/","section":"Tags","summary":"","title":"Cryptography"},{"content":"","date":null,"permalink":"/tags/distributed-systems/","section":"Tags","summary":"","title":"Distributed Systems"},{"content":"","date":null,"permalink":"/tags/technology/","section":"Tags","summary":"","title":"Technology"},{"content":"","date":null,"permalink":"/tags/aws/","section":"Tags","summary":"","title":"AWS"},{"content":"","date":null,"permalink":"/tags/career/","section":"Tags","summary":"","title":"Career"},{"content":"Three months at Amazon Web Services taught me more about real software engineering than two years of coursework. Not because school isn\u0026rsquo;t valuable‚Äîit absolutely is‚Äîbut because building systems that handle millions of dollars in enterprise contracts hits different than implementing data structures for homework.\nHere\u0026rsquo;s what I learned during my internship on the EC2 Private Pricing team, where I built an automated workflow system to extend AWS\u0026rsquo;s enterprise discount system from US-only to international markets.\nThe Reality of Big Tech Engineering Two-Pizza Teams Actually Work Amazon\u0026rsquo;s famous \u0026ldquo;two-pizza team\u0026rdquo; rule isn\u0026rsquo;t just corporate folklore. My team had eight people, and that small size meant:\nDirect access to senior engineers and principal architects Ownership of end-to-end features Quick decision-making without layers of bureaucracy Everyone knew what everyone else was working on The flip side? No hiding. Your contributions (or lack thereof) are immediately visible. There\u0026rsquo;s nowhere to coast.\nThe Development Rhythm Daily Standups (10 minutes, actually enforced) \u0026ldquo;What I did yesterday, what I\u0026rsquo;m doing today, any blockers.\u0026rdquo; That\u0026rsquo;s it. No status reports, no lengthy discussions. Blocked? We\u0026rsquo;d handle it after standup.\nTwo-Week Sprints Sprint planning on Monday, retrospective and demo on Friday of week two. The predictability helped me plan my work and manage expectations‚Äîboth my manager\u0026rsquo;s and my own.\nCode Reviews: The Real Education Every single line of production code needed review from a senior engineer. Initially intimidating, it became the most valuable part of my internship. Comments like:\n\u0026ldquo;This works, but have you considered the latency impact at scale?\u0026rdquo; \u0026ldquo;What happens when DynamoDB throttles this request?\u0026rdquo; \u0026ldquo;Let\u0026rsquo;s add metrics here‚Äîwe\u0026rsquo;ll want to monitor this in production\u0026rdquo; These weren\u0026rsquo;t nitpicks. They were lessons in thinking at AWS scale.\nDesigning for the Real World My First Design Review Disaster Week 2: I presented my initial design‚Äîa straightforward solution for handling Indian marketplace pricing. I\u0026rsquo;d spent days on it, feeling confident.\nThe feedback was brutal but constructive:\n\u0026ldquo;What about when we expand to Brazil?\u0026rdquo; \u0026ldquo;How does this handle currency fluctuations?\u0026rdquo; \u0026ldquo;What\u0026rsquo;s the rollback strategy if this fails?\u0026rdquo; \u0026ldquo;Where are the metrics and alarms?\u0026rdquo; I hadn\u0026rsquo;t thought about any of that. School projects succeed if they work. Production systems need to work, scale, fail gracefully, recover automatically, and integrate with existing infrastructure.\nThe Pivot That Changed Everything Week 4: Business requirements changed. Instead of just India, we needed to support any future international marketplace. My specific solution became useless overnight.\nThis is where my manager taught me a crucial lesson: \u0026ldquo;Design for change. The only constant at AWS is that requirements will evolve.\u0026rdquo;\nThe Second Design: Building for Tomorrow Armed with feedback and a broader scope, I took a different approach:\nBefore jumping to solutions, I asked:\nWhat problem are we actually solving? Who are our customers (internal and external)? What does success look like in metrics? What are the non-negotiables (latency, availability, security)? The new design principles:\nConfiguration over code (new marketplaces via config, not code changes) Modular components (each piece does one thing well) Comprehensive monitoring (know about issues before customers do) Gradual rollout capability (test with small traffic percentages) The stakeholder strategy: Instead of waiting for the formal review, I scheduled informal chats with:\nThe principal engineer (architecture patterns) The operations team (deployment and monitoring) The security team (compliance requirements) Sister teams (integration points) By the time the formal review came, I\u0026rsquo;d already addressed 90% of potential concerns. The review became a refinement session, not a redesign session.\nThe Code Quality Awakening School Code vs Production Code School:\ndef calculate_discount(price, discount_rate): return price * (1 - discount_rate) Production:\ndef calculate_discount( price: Decimal, discount_rate: Decimal, currency: str, customer_id: str, marketplace: str ) -\u0026gt; PricingResult: \u0026#34;\u0026#34;\u0026#34; Calculate customer-specific discount for a given marketplace. Args: price: Base price in specified currency discount_rate: Decimal between 0 and 1 currency: ISO 4217 currency code customer_id: Unique customer identifier marketplace: Target marketplace code Returns: PricingResult containing final price and audit metadata Raises: InvalidPriceError: If price is negative or exceeds limits InvalidDiscountError: If discount rate outside valid range MarketplaceNotSupportedError: If marketplace not configured \u0026#34;\u0026#34;\u0026#34; # Validate inputs _validate_price(price, currency, marketplace) _validate_discount_rate(discount_rate) _validate_marketplace_support(marketplace) # Log for audit trail logger.info(f\u0026#34;Calculating discount for customer={customer_id}, \u0026#34; f\u0026#34;marketplace={marketplace}, base_price={price}\u0026#34;) # Calculate with proper decimal precision final_price = price * (Decimal(\u0026#39;1\u0026#39;) - discount_rate) # Emit metrics metrics.record_pricing_calculation( marketplace=marketplace, customer_segment=_get_customer_segment(customer_id), discount_applied=float(discount_rate) ) return PricingResult( final_price=final_price, currency=currency, calculation_timestamp=datetime.utcnow(), audit_id=str(uuid.uuid4()) ) The difference? Production code assumes everything can and will go wrong.\nTesting: The Unsung Hero School projects: \u0026ldquo;It works on my machine!\u0026rdquo; AWS: \u0026ldquo;Here\u0026rsquo;s proof it works everywhere, always.\u0026rdquo;\nI wrote:\nUnit tests (every function, every edge case) Integration tests (components working together) Load tests (what happens at 1000 requests/second?) Failure tests (what if DynamoDB is down?) Rollback tests (can we revert safely?) The test code was 3x longer than the actual code. That\u0026rsquo;s normal.\nTime Management in the Real World The Milestone Mindset My manager introduced me to working backwards from deadlines:\nWeek 12: Code complete Week 11: Final testing and documentation Week 10: Code review and revisions Week 8-9: Implementation Week 6-7: Prototype and early feedback Week 3-5: Design and review Week 1-2: Onboarding and understanding context This wasn\u0026rsquo;t a suggestion‚Äîit was survival. Missing internship deadlines means no return offer.\nManaging Blockers The 30-Minute Rule: Stuck for 30 minutes? Reach out. Pride has no place when you\u0026rsquo;re on a deadline.\nMy escalation path:\nCheck documentation and code Ask team chat Direct message a teammate Schedule time with my mentor Escalate to manager I initially felt bad about asking questions. My mentor\u0026rsquo;s perspective: \u0026ldquo;You\u0026rsquo;re an intern. Not asking questions would be weird.\u0026rdquo;\nThe AI Revolution I Didn\u0026rsquo;t Expect Amazon\u0026rsquo;s internal AI tools transformed how I worked. Not ChatGPT‚Äîsomething more powerful because it understood Amazon\u0026rsquo;s codebase, patterns, and standards.\nUnderstanding existing code: \u0026ldquo;Explain what this Lambda function does and how it interacts with DynamoDB\u0026rdquo; Gets detailed explanation with sequence diagrams\nWriting new code: \u0026ldquo;Generate a Lambda handler that processes SNS messages for price updates, following team conventions\u0026rdquo; Gets production-ready code with error handling, logging, and metrics\nDebugging: \u0026ldquo;This DynamoDB query is timing out in production but works locally. What could cause this?\u0026rdquo; Gets list of likely causes ranked by probability\nThe productivity boost was insane. Tasks that would\u0026rsquo;ve taken hours took minutes. But here\u0026rsquo;s the key: AI accelerated my learning, it didn\u0026rsquo;t replace it. I still needed to understand why the AI\u0026rsquo;s suggestions worked and when they didn\u0026rsquo;t apply.\nThe Soft Skills Nobody Talks About Communication Is Code Writing clear Slack messages, emails, and documentation was as important as writing code. The template that worked:\nContext: What are you working on? Problem: What specific issue are you facing? What you\u0026rsquo;ve tried: Shows you\u0026rsquo;ve put in effort Specific ask: What exactly do you need?\nManaging Up Weekly 1:1s with my manager weren\u0026rsquo;t status updates‚Äîthose were in written reports. Instead:\nStrategic questions about approach Career development discussions Feedback on what I could improve Understanding team/company priorities The Feedback Loop Mid-internship review revealed I was too heads-down: \u0026ldquo;Your code is solid, but we don\u0026rsquo;t see you engaging with the broader team.\u0026rdquo;\nThe fix: I started attending optional architecture reviews, asking questions in team meetings, and sharing what I learned in team knowledge-sharing sessions.\nThe Seattle Experience Beyond work, Seattle was incredible. The AWS office had everything‚Äîfree food, gym, game rooms‚Äîbut what mattered was the people. My team grabbed lunch together daily, debating everything from system design to the best coffee shops.\nWeekends meant exploring Pike Place Market, hiking Rattlesnake Ledge, and yes, visiting the original Starbucks (overrated, but you have to do it).\nThe Lasting Impact This internship changed my perspective on software engineering:\nScale matters: A solution that works for 10 users might collapse at 10,000 Code is communication: Others will read, modify, and depend on your code Design for change: Requirements will evolve, plan for it Metrics over feelings: \u0026ldquo;I think it\u0026rsquo;s fast\u0026rdquo; vs \u0026ldquo;P99 latency is 47ms\u0026rdquo; Ownership mindset: You own your code in production, not just until merge Final Thoughts The internship wasn\u0026rsquo;t just about learning AWS services or writing production code. It was about understanding how world-class engineering organizations operate, how to think systematically about problems, and how to build solutions that scale.\nMost importantly, it confirmed what I suspected: I love this work. The complexity, the scale, the impact‚Äîbuilding systems that power the internet is incredibly fulfilling.\nTo future interns: embrace the discomfort of not knowing things, ask questions shamelessly, and remember that everyone‚Äîeven principal engineers‚Äîwas once where you are.\n","date":"1 September 2025","permalink":"/blog/aws-internship/","section":"Blog","summary":"Three months at Amazon Web Services taught me more about real software engineering than two years of coursework.","title":"Inside AWS: Lessons from My Internship"},{"content":"","date":null,"permalink":"/tags/internship/","section":"Tags","summary":"","title":"Internship"},{"content":"","date":null,"permalink":"/tags/software-engineering/","section":"Tags","summary":"","title":"Software Engineering"},{"content":"","date":null,"permalink":"/tags/cloud-computing/","section":"Tags","summary":"","title":"Cloud Computing"},{"content":"An in-depth exploration of cloud computing fundamentals, covering everything from basic concepts to advanced deployment strategies and containerization technologies.\nWhat is Cloud Computing? Cloud computing represents a fundamental shift in how we consume and deliver computing resources. Instead of maintaining physical servers and infrastructure, organizations can access computing power, storage, and applications on-demand over the internet, paying only for what they use.\nOn-Premise vs Cloud: The Paradigm Shift Traditional On-Premise Infrastructure:\nLimited Scalability ‚Äì Physical hardware constraints require significant lead time and capital investment to scale High Maintenance Overhead ‚Äì Organizations must handle hardware failures, software updates, and security patches Capital Expenditure Model ‚Äì Large upfront investments in servers, networking equipment, and data centers Geographic Limitations ‚Äì Collaboration restricted by physical location of infrastructure Disaster Recovery Challenges ‚Äì Complex and expensive backup solutions, often with single points of failure Cloud Computing Advantages:\nElastic Scalability ‚Äì Instantly scale resources up or down based on demand Managed Services ‚Äì Cloud providers handle infrastructure maintenance, updates, and physical security Operational Expenditure Model ‚Äì Pay-as-you-go pricing converts capital expenses to operational expenses Global Accessibility ‚Äì Access resources from anywhere with internet connectivity Built-in Redundancy ‚Äì Automatic backups, geo-replication, and disaster recovery capabilities Core Characteristics of Cloud Computing üìä On-Demand Self-Service\nProvision computing resources automatically without requiring human interaction with service providers. Spin up servers, configure networks, and deploy applications through web interfaces or APIs.\nüåê Broad Network Access\nResources accessible from anywhere using standard internet protocols. Support for diverse client platforms including mobile devices, tablets, laptops, and workstations.\nüîÑ Resource Pooling\nMulti-tenant model where physical and virtual resources are dynamically assigned based on demand. Location independence allows resources to be accessed regardless of physical data center location.\n‚ö° Rapid Elasticity\nCapabilities can be elastically provisioned and released to scale with demand. From the consumer\u0026rsquo;s perspective, resources appear unlimited and can be purchased in any quantity at any time.\nüìà Measured Service\nResource usage monitored, controlled, and reported transparently. Pay only for resources actually consumed with detailed billing breakdowns.\nDeployment Models Public Cloud The Shared Infrastructure Model\nLike using public transportation‚Äîefficient, cost-effective, but shared with others. Resources owned and operated by third-party cloud service providers and delivered over the internet.\nBenefits:\nNo capital expenditure on hardware Reduced operational costs Near-unlimited scalability High reliability with extensive redundancy Best For: Startups, development/testing environments, websites with variable traffic, SaaS applications\nMajor Providers: AWS, Microsoft Azure, Google Cloud Platform, IBM Cloud\nPrivate Cloud The Dedicated Infrastructure Model\nLike owning a private vehicle‚Äîcomplete control but higher costs. Infrastructure used exclusively by a single organization, either on-premise or hosted by a third party.\nBenefits:\nEnhanced security and privacy Greater control over infrastructure Customizable to specific organizational needs Compliance with strict regulatory requirements Best For: Government agencies, financial institutions, healthcare organizations, enterprises with specific compliance needs\nHybrid Cloud The Best of Both Worlds\nCombines public and private clouds with orchestration between platforms. Allows data and applications to move between private and public clouds for greater flexibility.\nBenefits:\nKeep sensitive data on-premise while leveraging public cloud for less-sensitive operations Cloud bursting for handling traffic spikes Gradual cloud migration path Cost optimization through strategic workload placement Best For: Organizations with regulatory requirements, legacy systems integration, variable workloads\nMulti-Cloud The Diversified Approach\nUsing multiple cloud computing services from different providers simultaneously. Prevents vendor lock-in and leverages best-of-breed services.\nBenefits:\nAvoid vendor lock-in Leverage specialized services from different providers Geographic distribution for compliance Increased resilience and redundancy Service Models Infrastructure as a Service (IaaS) The Foundation Layer\nProvides virtualized computing resources over the internet. Users manage applications, data, runtime, middleware, and OS while the provider manages virtualization, servers, storage, and networking.\nKey Components:\nVirtual Machines with customizable CPU, memory, and storage Load balancers for distributing traffic Virtual networks and firewalls Block and object storage systems Examples: Amazon EC2, Google Compute Engine, Azure Virtual Machines, DigitalOcean Droplets\nUse Cases: Website hosting, big data analysis, backup and recovery, high-performance computing\nPlatform as a Service (PaaS) The Development Layer\nProvides a platform for developers to build, run, and manage applications without dealing with infrastructure complexity.\nKey Features:\nPre-configured runtime environments Integrated development tools and databases Automatic scaling and load balancing Built-in security and compliance features Examples: Google App Engine, Azure App Service, Heroku, AWS Elastic Beanstalk\nUse Cases: API development, microservices, web applications, mobile backends\nSoftware as a Service (SaaS) The Application Layer\nComplete applications delivered over the internet on a subscription basis. Users access software through web browsers without installation or maintenance.\nCharacteristics:\nCentrally hosted and managed Automatic updates and patch management Accessible from any device Subscription-based pricing Examples: Google Workspace, Microsoft 365, Salesforce, Slack, Zoom\nUse Cases: Email and collaboration, CRM, HR management, accounting software\nFunction as a Service (FaaS) / Serverless The Evolution of Cloud Services\nExecute code in response to events without managing servers. Automatically scales and charges only for actual compute time used.\nBenefits:\nNo server management Automatic scaling Pay per execution Built-in high availability Examples: AWS Lambda, Azure Functions, Google Cloud Functions\nUse Cases: Real-time file processing, IoT data processing, API backends, scheduled tasks\nMajor Cloud Providers Amazon Web Services (AWS) The Market Leader\nLaunched in 2006, AWS offers over 200 fully-featured services from data centers globally.\nKey Services:\nCompute: EC2, Lambda, ECS, EKS Storage: S3, EBS, EFS, Glacier Database: RDS, DynamoDB, Redshift Networking: VPC, CloudFront, Route 53 AI/ML: SageMaker, Rekognition, Comprehend Strengths: Largest service portfolio, mature ecosystem, extensive documentation\nMicrosoft Azure The Enterprise Choice\nStrong integration with Microsoft\u0026rsquo;s enterprise software stack and hybrid cloud capabilities.\nKey Services:\nCompute: Virtual Machines, Functions, Container Instances Storage: Blob Storage, File Storage, Queue Storage Database: SQL Database, Cosmos DB, Database for PostgreSQL AI/ML: Machine Learning Studio, Cognitive Services Strengths: Enterprise integration, hybrid cloud, strong PaaS offerings\nGoogle Cloud Platform (GCP) The Innovation Platform\nLeverages Google\u0026rsquo;s expertise in data analytics, machine learning, and containerization.\nKey Services:\nCompute: Compute Engine, Cloud Functions, GKE Storage: Cloud Storage, Persistent Disk Database: Cloud SQL, Firestore, Bigtable AI/ML: AutoML, Vision AI, Natural Language AI Strengths: Data analytics, machine learning, Kubernetes expertise\nCloud Architecture Best Practices Design Principles üèóÔ∏è Design for Failure\nAssume components will fail and design systems to handle failures gracefully. Implement redundancy, automated health checks, and self-healing mechanisms.\nüì¶ Loose Coupling\nReduce interdependencies between components. Use message queues, load balancers, and service discovery to enable independent scaling and updates.\nüîê Security in Depth\nImplement multiple layers of security controls. Use encryption at rest and in transit, implement least privilege access, and enable comprehensive logging.\nüí∞ Cost Optimization\nRight-size resources, use reserved instances for predictable workloads, implement auto-scaling, and regularly review and optimize resource utilization.\nThe Well-Architected Framework Operational Excellence\nInfrastructure as Code Automated deployments Monitoring and logging Incident response procedures Security\nIdentity and access management Data protection Infrastructure protection Incident detection and response Reliability\nDistributed system design Recovery planning Scaling strategies Change management Performance Efficiency\nResource selection Performance monitoring Load testing Continuous optimization Cost Optimization\nExpenditure awareness Cost-effective resources Matching supply with demand Optimization over time Containerization and Orchestration Docker The Container Revolution\nDocker packages applications with their dependencies into portable containers that run consistently across environments.\nKey Concepts:\nImages: Read-only templates containing application code and dependencies Containers: Runnable instances of images Dockerfile: Text files defining how to build images Registry: Repository for storing and distributing images Kubernetes The Orchestration Standard\nOpen-source platform for automating deployment, scaling, and management of containerized applications.\nCore Components:\nPods: Smallest deployable units containing one or more containers Services: Stable networking endpoints for accessing pods Deployments: Declarative updates for pods and ReplicaSets Ingress: External access to services within a cluster Cloud-Native Development Microservices Architecture\nDecompose applications into small, independent services that communicate through APIs. Each service can be developed, deployed, and scaled independently.\nDevOps Integration\nCombine development and operations practices using CI/CD pipelines, infrastructure as code, and automated testing to accelerate delivery.\nService Mesh\nInfrastructure layer for handling service-to-service communication. Tools like Istio provide traffic management, security, and observability.\nFuture Trends ü§ñ AI/ML Integration\nCloud platforms increasingly offering pre-trained models, AutoML capabilities, and managed ML infrastructure.\nüîí Zero-Trust Security\nMoving beyond perimeter-based security to verify every transaction regardless of source.\nüåç Edge Computing\nProcessing data closer to where it\u0026rsquo;s generated, reducing latency and bandwidth usage.\nüå± Sustainable Computing\nCloud providers investing in renewable energy and carbon-neutral operations.\n‚ö° Quantum Computing\nCloud-based quantum computing services becoming available for specialized workloads.\nConclusion Cloud computing has transformed from a buzzword to the foundation of modern IT infrastructure. Understanding its principles, services, and best practices is essential for anyone working in technology today. Whether you\u0026rsquo;re building a startup, modernizing enterprise systems, or developing the next breakthrough application, cloud computing provides the flexibility, scalability, and innovation platform necessary for success in the digital age.\nThis guide serves as a comprehensive introduction to cloud computing. As the field rapidly evolves, stay updated with the latest developments from cloud providers and the Cloud Native Computing Foundation (CNCF).\n","date":"20 April 2025","permalink":"/blog/cloud/","section":"Blog","summary":"An in-depth exploration of cloud computing fundamentals, covering everything from basic concepts to advanced deployment strategies and containerization technologies.","title":"Cloud Computing Fundamentals"},{"content":"","date":null,"permalink":"/tags/containers/","section":"Tags","summary":"","title":"Containers"},{"content":"","date":null,"permalink":"/tags/devops/","section":"Tags","summary":"","title":"DevOps"},{"content":"","date":null,"permalink":"/tags/infrastructure/","section":"Tags","summary":"","title":"Infrastructure"},{"content":"","date":null,"permalink":"/tags/backend-development/","section":"Tags","summary":"","title":"Backend Development"},{"content":"","date":null,"permalink":"/tags/mongodb/","section":"Tags","summary":"","title":"MongoDB"},{"content":"","date":null,"permalink":"/tags/node.js/","section":"Tags","summary":"","title":"Node.js"},{"content":"","date":null,"permalink":"/tags/rest-api/","section":"Tags","summary":"","title":"REST API"},{"content":"Let\u0026rsquo;s talk about building RESTful APIs that don\u0026rsquo;t fall apart in production. After spending months building and rebuilding backend services, I\u0026rsquo;ve learned that creating an API is easy‚Äîcreating one that\u0026rsquo;s secure, maintainable, and performs well under pressure is where the real challenge lies.\nWhat Even Is a RESTful API? Before diving into the implementation details, let\u0026rsquo;s demystify this term. An API (Application Programming Interface) is essentially a contract between different pieces of software, allowing them to communicate. REST (Representational State Transfer) is just a set of conventions that makes these APIs predictable and easy to work with.\nThink of it like ordering at a restaurant. You (the client) look at a menu (API documentation), place an order using specific terms the waiter understands (HTTP methods), and receive your food in a predictable format (JSON response). The kitchen doesn\u0026rsquo;t need to know who you are between visits (stateless), and the menu items are organized logically (resources).\nThe REST Principles That Actually Matter After building several APIs, I\u0026rsquo;ve found these five principles make the biggest difference:\n1. Resources Are King Instead of thinking about actions, think about resources. Don\u0026rsquo;t create endpoints like:\nPOST /createNewUser GET /getAllProjects POST /updateUserEmail Instead, organize around resources:\nPOST /users (create user) GET /projects (get all projects) PATCH /users/123 (update user) This mental shift from \u0026ldquo;what action am I performing\u0026rdquo; to \u0026ldquo;what resource am I manipulating\u0026rdquo; makes your API intuitive.\n2. HTTP Methods Have Meaning Each HTTP method has a specific purpose:\nGET: Reading data (never changes anything) POST: Creating new resources PUT: Replacing entire resources PATCH: Updating parts of resources DELETE: Removing resources Here\u0026rsquo;s a mistake I made early on: using POST for everything because \u0026ldquo;it works.\u0026rdquo; Sure, it works, but when another developer (or future you) tries to understand the API, they\u0026rsquo;ll curse your name.\n3. Statelessness Isn\u0026rsquo;t Optional Every request must contain all information needed to understand it. The server shouldn\u0026rsquo;t remember previous requests. This sounds simple until you\u0026rsquo;re tempted to store user session data on the server \u0026ldquo;just this once.\u0026rdquo; Don\u0026rsquo;t. Use JWTs or similar tokens to maintain state on the client side.\nThe MVC Pattern: Keeping Your Sanity When I first started, my route files were a disaster‚Äîdatabase queries mixed with business logic mixed with response formatting. Enter the MVC pattern:\n// Model (models/userModel.js) // Handles data and business logic const User = { async create(userData) { // Validation logic if (!userData.email) throw new Error(\u0026#39;Email required\u0026#39;); // Database interaction return await db.users.insert(userData); }, }; // Controller (controllers/userController.js) // Handles requests and responses const createUser = async (req, res) =\u0026gt; { try { const user = await User.create(req.body); res.status(201).json({ status: \u0026#39;success\u0026#39;, data: user }); } catch (error) { res.status(400).json({ status: \u0026#39;error\u0026#39;, message: error.message }); } }; // Routes (routes/userRoutes.js) // Defines endpoints router.post(\u0026#39;/users\u0026#39;, createUser); Fat models, thin controllers became my mantra. Push business logic into models, keep controllers focused on handling HTTP concerns.\nMongoDB \u0026amp; Mongoose: The Dynamic Duo Coming from SQL, MongoDB felt like freedom‚Äîno rigid schemas, nested documents, easy scaling. But with great power comes great responsibility. Here\u0026rsquo;s what I learned:\nSchema Design Matters (Even in NoSQL) Just because MongoDB is schemaless doesn\u0026rsquo;t mean your data should be. Mongoose schemas saved me from myself:\nconst projectSchema = new mongoose.Schema({ name: { type: String, required: [true, \u0026#39;Project must have a name\u0026#39;], trim: true, maxlength: [100, \u0026#39;Name too long\u0026#39;], }, status: { type: String, enum: [\u0026#39;active\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;archived\u0026#39;], default: \u0026#39;active\u0026#39;, }, team: [ { type: mongoose.Schema.ObjectId, ref: \u0026#39;User\u0026#39;, }, ], }); The Reference vs Embed Dilemma This drove me crazy initially. When should you embed documents vs reference them?\nEmbed when:\nData is accessed together (user profile + preferences) One-to-few relationships (blog post + comments) Data won\u0026rsquo;t grow unbounded Reference when:\nData needs independent queries (users in a project) Many-to-many relationships Large documents that change frequently I learned this the hard way when I embedded all project tickets inside project documents. Worked great until projects had thousands of tickets and every query returned megabytes of data.\nSecurity: Where Things Get Serious Security isn\u0026rsquo;t optional, and \u0026ldquo;I\u0026rsquo;ll add it later\u0026rdquo; is a dangerous game. Here\u0026rsquo;s my security checklist that\u0026rsquo;s saved me multiple times:\nPassword Security: Beyond Bcrypt While everyone talks about bcrypt, I switched to Argon2 after diving into the research. It\u0026rsquo;s memory-hard, making GPU attacks impractical:\nconst argon2 = require(\u0026#39;argon2\u0026#39;); // Hashing const hash = await argon2.hash(password, { type: argon2.argon2id, memoryCost: 2 ** 16, timeCost: 3, parallelism: 1, }); // Verifying const valid = await argon2.verify(hash, password); JWT Implementation That Actually Works JWTs are powerful but easy to mess up. Store them in httpOnly cookies, not localStorage:\nconst token = jwt.sign({ userId: user._id }, process.env.JWT_SECRET, { expiresIn: \u0026#39;7d\u0026#39;, }); res.cookie(\u0026#39;jwt\u0026#39;, token, { httpOnly: true, secure: process.env.NODE_ENV === \u0026#39;production\u0026#39;, sameSite: \u0026#39;strict\u0026#39;, maxAge: 7 * 24 * 60 * 60 * 1000, }); The Security Measures That Saved Me Rate Limiting: Stopped a DDoS attempt cold const limiter = rateLimit({ max: 100, windowMs: 15 * 60 * 1000, message: \u0026#39;Too many requests\u0026#39;, }); app.use(\u0026#39;/api\u0026#39;, limiter); Input Sanitization: Prevented NoSQL injection const mongoSanitize = require(\u0026#39;express-mongo-sanitize\u0026#39;); app.use(mongoSanitize()); Security Headers: Basic but effective const helmet = require(\u0026#39;helmet\u0026#39;); app.use(helmet()); Performance Optimizations That Matter Indexing: The Low-Hanging Fruit Adding indexes to frequently queried fields improved response times by 10x:\nprojectSchema.index({ status: 1, createdAt: -1 }); projectSchema.index({ \u0026#39;team.userId\u0026#39;: 1 }); But remember: indexes use memory and slow down writes. Profile your queries first:\nconst explanation = await Project.find({ status: \u0026#39;active\u0026#39; }).explain( \u0026#39;executionStats\u0026#39; ); console.log(explanation.executionStats); The Aggregation Pipeline Magic Instead of fetching all documents and processing in Node.js, use MongoDB\u0026rsquo;s aggregation pipeline:\nconst stats = await Project.aggregate([ { $match: { status: \u0026#39;active\u0026#39; } }, { $unwind: \u0026#39;$team\u0026#39; }, { $group: { _id: \u0026#39;$team\u0026#39;, projectCount: { $sum: 1 }, avgCompletion: { $avg: \u0026#39;$completionRate\u0026#39; }, }, }, { $sort: { projectCount: -1 } }, ]); Caching Strategy (My TODO That Became Reality) I procrastinated on caching until our API started timing out. Redis solved it:\nconst cached = await redis.get(`project:${id}`); if (cached) return JSON.parse(cached); const project = await Project.findById(id); await redis.setex(`project:${id}`, 3600, JSON.stringify(project)); return project; Lessons from Production Disasters The Cascading Delete Nightmare I once deleted a user and watched in horror as all their projects, tickets, and comments vanished. Now I use soft deletes:\nuserSchema.add({ isActive: { type: Boolean, default: true }, }); userSchema.pre(/^find/, function () { this.find({ isActive: { $ne: false } }); }); The Factory Function Pattern That Saved My Controllers Repeating try-catch blocks everywhere was painful. Factory functions cleaned it up:\nconst catchAsync = (fn) =\u0026gt; { return (req, res, next) =\u0026gt; { fn(req, res, next).catch(next); }; }; // Clean controller without try-catch const getProject = catchAsync(async (req, res, next) =\u0026gt; { const project = await Project.findById(req.params.id); if (!project) { return next(new AppError(\u0026#39;Project not found\u0026#39;, 404)); } res.json({ status: \u0026#39;success\u0026#39;, data: project }); }); Client-Side Rendering for Dynamic Content Loading all comments with the initial page load was killing performance. Moving to client-side rendering for comments cut initial load time by 40%:\n// Instead of server-side rendering all comments // Load them dynamically when needed async function loadComments(ticketId) { const response = await fetch(`/api/tickets/${ticketId}/comments`); const comments = await response.json(); renderComments(comments.data); } The Tools That Make Life Easier After trying different setups, here\u0026rsquo;s my go-to stack:\nExpress.js: Still the most flexible Node.js framework MongoDB + Mongoose: Perfect for rapidly evolving schemas Pug: For server-side rendering (yes, better than Handlebars) Morgan: HTTP request logger that\u0026rsquo;s saved debugging hours PM2: Process manager for production Postman: API testing and documentation Looking Back: What I\u0026rsquo;d Do Differently Start with TypeScript: The type safety would\u0026rsquo;ve prevented countless runtime errors Write tests first: TDD feels slow initially but saves time long-term Document as you go: Your future self will thank you Use DataLoader pattern: Solves N+1 query problems elegantly Implement observability early: Logs, metrics, and traces from day one Final Thoughts Building RESTful APIs is a journey of continuous learning. Every production issue teaches you something new, every security breach you read about adds to your checklist, and every performance bottleneck makes you a better developer.\nThe key is starting simple and iterating. Don\u0026rsquo;t try to implement every best practice from day one‚Äîyou\u0026rsquo;ll never ship. Start with the basics: clean separation of concerns, basic security, and consistent patterns. Then improve incrementally.\nRemember: the best API is not the one with the most features or the cleverest abstractions. It\u0026rsquo;s the one that\u0026rsquo;s still running smoothly at 3 AM when you\u0026rsquo;re fast asleep, handling thousands of requests without breaking a sweat.\nCurrently working on: Implementing GraphQL alongside REST, exploring Deno as an alternative to Node.js, and diving deeper into microservices architecture. The learning never stops.\n","date":"20 January 2025","permalink":"/blog/api/","section":"Blog","summary":"Let\u0026rsquo;s talk about building RESTful APIs that don\u0026rsquo;t fall apart in production.","title":"RESTful APIs"},{"content":"","date":null,"permalink":"/tags/security/","section":"Tags","summary":"","title":"Security"},{"content":"","date":null,"permalink":"/tags/developer-tools/","section":"Tags","summary":"","title":"Developer Tools"},{"content":"","date":null,"permalink":"/tags/git/","section":"Tags","summary":"","title":"Git"},{"content":"Let\u0026rsquo;s be honest about Git: everyone pretends to understand it, but most of us are secretly terrified we\u0026rsquo;ll accidentally delete the entire codebase with one wrong command. If you\u0026rsquo;ve ever stared at a merge conflict like it\u0026rsquo;s written in ancient hieroglyphics, this post is for you.\nWhy Git Feels Like Dark Magic When I first encountered Git, the tutorials made it sound simple: \u0026ldquo;Just commit your changes and push!\u0026rdquo; What they didn\u0026rsquo;t mention was the existential crisis I\u0026rsquo;d face when encountering my first detached HEAD state, or the panic of realizing I\u0026rsquo;d been committing to the wrong branch for three days.\nThe truth is, Git is conceptually simple but practically complex. It\u0026rsquo;s like chess‚Äîyou can learn how the pieces move in minutes, but mastering the game takes years.\nThe Mental Model That Finally Clicked After countless botched merges and accidental force pushes, here\u0026rsquo;s the mental model that finally made Git make sense:\nGit is a time machine for your code. Every commit is a snapshot in time. Branches are alternate timelines. Merging is combining timelines. Rebasing is rewriting history (with all the dangers that implies).\nOnce I started thinking this way, commands started making intuitive sense:\ngit checkout = travel to a different point in time git reset --hard = forcefully travel back in time (destroying everything that came after) git stash = put current changes in a pocket dimension for safekeeping The Commands You Actually Need Forget the 150+ Git commands. Here\u0026rsquo;s what you\u0026rsquo;ll use 95% of the time:\nThe Daily Workflow git status # What\u0026#39;s going on? git add . # Stage everything (or be specific) git commit -m \u0026#34;message\u0026#34; # Save snapshot git pull # Get latest changes git push # Share your changes The \u0026ldquo;Oh No\u0026rdquo; Commands git diff # What did I change? git log --oneline -10 # What happened recently? git checkout -- filename # Undo changes to specific file git reset HEAD~1 # Undo last commit (keep changes) git reset --hard HEAD~1 # Nuclear option: destroy last commit The Collaboration Commands git checkout -b feature-name # Create new timeline git merge main # Bring in changes from main git rebase main # Alternative to merge (rewrites history) git cherry-pick \u0026lt;commit-id\u0026gt; # Steal one commit from another branch Branches: Your Safety Net Here\u0026rsquo;s what nobody told me about branches: they\u0026rsquo;re free, disposable, and your best friend. Create branches liberally:\ngit checkout -b experiment/crazy-idea # Go wild, break things # If it works: merge it # If it doesn\u0026#39;t: delete it and pretend it never happened The branching epiphany came when I realized branches aren\u0026rsquo;t precious. They\u0026rsquo;re scratch paper. Make a mess, try things out, throw them away if they don\u0026rsquo;t work.\nThe Merge Conflict Survival Guide Merge conflicts aren\u0026rsquo;t errors‚Äîthey\u0026rsquo;re Git asking for your help. Here\u0026rsquo;s the systematic approach that turned panic into process:\nDon\u0026rsquo;t panic (easier said than done) Understand what happened: Two people changed the same part of the same file Open the conflicted file and look for: \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD Your version ======= Their version \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; branch-name Decide what to keep: Sometimes it\u0026rsquo;s yours, sometimes theirs, often both Remove the markers and save Add and commit the resolution Pro tip: Use a merge tool like VS Code\u0026rsquo;s built-in merger. It turns cryptic conflicts into visual choices.\nRebasing: The Sharp Knife Rebasing is Git\u0026rsquo;s most powerful and dangerous feature. It literally rewrites history. Use it for:\nCleaning up messy commit history before merging Keeping feature branches up-to-date with main Making your commits look like you knew what you were doing all along # Interactive rebase to clean up last 3 commits git rebase -i HEAD~3 But remember the golden rule: Never rebase commits that exist outside your local repository. If you\u0026rsquo;ve pushed it, don\u0026rsquo;t rebase it (unless you really know what you\u0026rsquo;re doing).\nThe Mistakes That Taught Me the Most The Force Push Disaster I once force-pushed to main and wiped out a colleague\u0026rsquo;s entire day of work. Lesson learned:\nNever force push to shared branches If you must force push, use --force-with-lease (it\u0026rsquo;s safer) Communicate with your team The Commit Message Regret Early commits looked like:\n\u0026#34;fix\u0026#34; \u0026#34;updates\u0026#34; \u0026#34;asdfasdf\u0026#34; \u0026#34;finally works\u0026#34; Now I follow this format:\ntype: brief description Longer explanation if needed - Bullet points for multiple changes - Reference issue numbers (#123) The Binary File Bloat Committed a 100MB test video. Repository became sluggish forever. Lessons:\nUse .gitignore religiously Think twice before committing binaries Large files belong in Git LFS or somewhere else entirely Git in the Real World The Pull Request Dance Real development isn\u0026rsquo;t just committing to main. It\u0026rsquo;s:\nCreate feature branch Make changes Push branch Open pull request Address review comments Squash and merge The Unwritten Rules Commit early and often (you can clean up history later) Write commit messages for your future confused self Pull before you push (avoid unnecessary conflicts) When in doubt, make a backup branch git reflog is your safety net‚Äîit tracks everything Advanced Techniques Worth Learning Stashing: Your Quick Save git stash # Quick save current changes git stash pop # Restore and delete stash git stash apply # Restore but keep stash git stash list # See all stashes Bisect: Binary Search for Bugs git bisect start git bisect bad # Current commit is broken git bisect good \u0026lt;commit-hash\u0026gt; # This commit worked # Git checks out commits for you to test # Keep marking as good/bad until bug commit found Worktrees: Multiple Branches Simultaneously git worktree add ../project-hotfix hotfix-branch # Now you have two folders with different branches checked out The Path to Git Confidence Git mastery isn\u0026rsquo;t about memorizing commands‚Äîit\u0026rsquo;s about understanding the model and knowing how to recover from mistakes. Here\u0026rsquo;s the progression:\nFear Stage: Copy-pasting commands, praying nothing breaks Cautious Stage: Understanding basic commands, still scared of rebasing Confident Stage: Comfortable with branches, merging, and basic recovery Mastery Stage: Rebasing interactively, bisecting bugs, teaching others Most developers hover between stages 2 and 3, and that\u0026rsquo;s perfectly fine.\nFinal Thoughts Git is a powerful tool that becomes less intimidating with practice. Every developer has Git horror stories‚Äîforce pushing to main, losing work, spending hours on merge conflicts. These aren\u0026rsquo;t failures; they\u0026rsquo;re rites of passage.\nThe secret to Git confidence isn\u0026rsquo;t avoiding mistakes‚Äîit\u0026rsquo;s knowing how to recover from them. Keep a learning mindset, experiment in safe environments, and remember: someone, somewhere, has made a worse Git mistake than you\u0026rsquo;re about to make.\n","date":"10 November 2024","permalink":"/blog/git/","section":"Blog","summary":"Let\u0026rsquo;s be honest about Git: everyone pretends to understand it, but most of us are secretly terrified we\u0026rsquo;ll accidentally delete the entire codebase with one wrong command.","title":"Git Fundamentals"},{"content":"","date":null,"permalink":"/tags/version-control/","section":"Tags","summary":"","title":"Version Control"},{"content":"","date":null,"permalink":"/tags/ai-tools/","section":"Tags","summary":"","title":"AI Tools"},{"content":"A project management platform designed to streamline how development teams work together by bringing task tracking, team communication, and project analytics into one unified workspace.\nThe Problem Development teams often juggle multiple disconnected tools‚Äîone for messaging, another for task tracking, and yet another for metrics. This fragmentation creates information gaps and makes it harder to maintain project visibility. Atelier addresses this by providing a single platform where teams can manage their entire workflow.\nKey Features üîê Authentication \u0026amp; Access Control\nArgon2 password hashing with JWT token-based sessions. Role-based permissions system (Admin, Project Manager, Developer, Team Member) controls access to project resources. Automated email notifications for password resets and task assignments.\nüìä Real-Time Collaboration\nShared Kanban boards with instant updates via WebSocket connections‚Äîdrag a task and everyone sees it move immediately. Integrated chat keeps discussions in context with the relevant tasks. Visual analytics track sprint progress and identify bottlenecks.\n‚ö° Technical Architecture\nNode.js/Express.js backend following MVC patterns with RESTful APIs. MongoDB for flexible document storage using Atlas for cloud hosting. Optimized database queries with proper indexing and efficient real-time event handling through Socket.io.\nTechnical Implementation The tech stack was chosen for practical reasons: Node.js handles concurrent connections well (crucial for real-time features), MongoDB allows schema flexibility during rapid development, and JWT authentication keeps the system stateless and scalable.\nEach API endpoint validates permissions through custom middleware. When someone updates a Kanban board, WebSocket events broadcast minimal data updates rather than full refreshes, keeping the system responsive even with multiple concurrent users.\nChallenges \u0026amp; Solutions Challenge: Synchronizing multiple clients without overwhelming the server\nSolution: Implemented event throttling and delta updates‚Äîsending only changed data instead of full state refreshes\nChallenge: Managing complex permission logic across different roles\nSolution: Centralized authorization middleware that validates permissions before any database operation\nCurrent Status Working Features:\nUser registration/login with secure authentication Project creation and team member management Drag-and-drop Kanban boards with real-time updates Basic chat system within project context Role-based access control across all features In Development:\nEnhanced analytics dashboard with actionable metrics Performance optimization for larger teams Mobile responsive UI improvements This ongoing project demonstrates practical application of full-stack development concepts while solving real team collaboration challenges.\n","date":"14 March 2024","permalink":"/projects/atelier/","section":"Projects","summary":"A project management platform designed to streamline how development teams work together by bringing task tracking, team communication, and project analytics into one unified workspace.","title":"Atelier"},{"content":"","date":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning"},{"content":" From ideas to reality ","date":null,"permalink":"/projects/","section":"Projects","summary":" From ideas to reality ","title":"Projects"},{"content":"When Cognition Labs announced Devin as the \u0026ldquo;world\u0026rsquo;s first AI software engineer,\u0026rdquo; the tech community\u0026rsquo;s reaction ranged from excitement to existential dread. After digging into the available information and comparing it with existing AI development tools, here\u0026rsquo;s my take on what Devin actually represents‚Äîand what it doesn\u0026rsquo;t.\nWhat We Actually Know About Devin Let\u0026rsquo;s start with the facts. Despite the marketing buzz, concrete technical details about Devin remain surprisingly scarce. Based on Cognition\u0026rsquo;s demonstrations and limited public information, we can infer Devin likely combines:\nNatural Language Understanding\nDevin interprets high-level project descriptions and converts them into technical requirements. This goes beyond simple prompt-to-code translation‚Äîit appears to understand project context and can maintain that context across multiple development tasks.\nCode Generation and Analysis\nUsing large language models trained on code repositories, Devin generates functional code across multiple files and frameworks. The demonstrations show it working with Python web applications, JavaScript frontends, and various APIs.\nDevelopment Environment Integration\nUnlike ChatGPT or Copilot, Devin operates within its own development environment, complete with a code editor, terminal, and browser. This allows it to run commands, test code, and debug issues autonomously.\nThe Claims vs. Reality Cognition makes bold claims about Devin\u0026rsquo;s capabilities. Let\u0026rsquo;s examine them critically:\n\u0026ldquo;Completes Real Freelance Jobs\u0026rdquo; The demo shows Devin completing an Upwork project, but we need context. The task involved relatively straightforward computer vision work‚Äîrunning inference on images using pre-existing models. While impressive, this represents a narrow slice of real-world software engineering.\n\u0026ldquo;Resolves 13.86% of GitHub Issues\u0026rdquo; This benchmark from the SWE-bench dataset sounds significant until you consider that most existing AI tools score near 0%. However, the dataset consists of Python repository issues, and we don\u0026rsquo;t know:\nThe complexity distribution of solved issues How many attempts Devin needed Whether solutions were production-ready or just passed tests \u0026ldquo;Plans and Executes Complex Tasks\u0026rdquo; The demonstrations show Devin breaking down projects into steps, but the examples are relatively linear workflows. Real software projects involve complex decision trees, stakeholder feedback loops, and architectural trade-offs that weren\u0026rsquo;t demonstrated.\nHow Devin Actually Differs from Current Tools Having used GitHub Copilot, ChatGPT, and other AI coding assistants extensively, here\u0026rsquo;s where Devin genuinely breaks new ground:\nPersistent Context and State\nWhile ChatGPT loses context after a conversation ends and Copilot only sees your current file, Devin maintains project state across entire development sessions. This is crucial for multi-file projects.\nAutonomous Execution\nCurrent tools require constant human supervision‚Äîcopy-pasting code, running commands, interpreting errors. Devin can execute its own code, read error messages, and attempt fixes without intervention.\nTool Integration\nDevin can allegedly use developer tools like browsers for testing, API documentation for reference, and deployment platforms. This toolchain integration is beyond what current AI assistants offer.\nThe Technical Limitations Nobody\u0026rsquo;s Discussing The Context Window Problem Even advanced LLMs have context limitations. Large codebases with hundreds of files and complex dependencies likely exceed Devin\u0026rsquo;s ability to maintain full project understanding. How does it handle a million-line enterprise application?\nThe Debugging Paradox Debugging often requires understanding not just code, but the business logic, user intentions, and edge cases that led to the bug. Devin might fix syntax errors and obvious logical flaws, but subtle race conditions or business logic errors require human insight.\nThe Architecture Dilemma Software architecture involves trade-offs between performance, maintainability, cost, and scalability. These decisions require understanding business constraints, future growth projections, and team capabilities‚Äîcontext that Devin cannot fully grasp.\nWhat This Actually Means for Developers The Good Automation of Boilerplate\nSetting up new projects, writing CRUD operations, implementing standard authentication‚Äîthese repetitive tasks are perfect for AI automation.\nEnhanced Debugging\nHaving an AI that can systematically test hypotheses and isolate issues could significantly speed up debugging, especially for reproducible bugs.\nLearning Acceleration\nJunior developers could learn faster by watching Devin work, understanding patterns, and getting explanations for implementation choices.\nThe Concerning Quality vs. Speed Trade-off\nAI-generated code often works but isn\u0026rsquo;t optimal. Without careful review, technical debt could accumulate rapidly.\nOverreliance Risk\nDevelopers who lean too heavily on AI tools might not develop deep problem-solving skills or understanding of underlying systems.\nSecurity Implications\nAI models trained on public code might inadvertently introduce vulnerabilities or use outdated, insecure patterns.\nThe Real Paradigm Shift The significance of Devin isn\u0026rsquo;t that it will replace programmers‚Äîit won\u0026rsquo;t. The real shift is in how we conceptualize development work:\nFrom Coding to Orchestration\nDevelopers might spend less time writing code and more time reviewing, refining, and orchestrating AI-generated solutions.\nHigher Abstraction Levels\nWe could work at increasingly higher levels of abstraction, describing what we want rather than how to implement it.\nEmphasis on Soft Skills\nUnderstanding requirements, communicating with stakeholders, and making architectural decisions become even more valuable as coding becomes commoditized.\nWhat\u0026rsquo;s Missing from the Conversation Economic Impact If AI can handle junior-level tasks, how does this affect entry-level positions? Will the barrier to entry rise as companies expect developers to provide value beyond what AI can deliver?\nCode Ownership and Liability Who\u0026rsquo;s responsible when AI-generated code causes problems? How do we handle intellectual property when code is generated by models trained on open-source repositories?\nEnvironmental Cost Training and running these models requires significant computational resources. What\u0026rsquo;s the carbon footprint of AI-assisted development compared to traditional programming?\nMoving Forward: A Pragmatic Approach Instead of fearing or hyping Devin, here\u0026rsquo;s how developers should respond:\nTreat it as a tool, not a threat‚ÄîLike IDEs and frameworks before it, AI is another tool in our toolkit\nFocus on skills AI can\u0026rsquo;t replicate‚ÄîSystem design, stakeholder communication, creative problem-solving, and ethical decision-making\nStay curious but critical‚ÄîExperiment with AI tools but understand their limitations and verify their output\nContribute to the discussion‚ÄîAs developers, we should actively shape how these tools evolve rather than passively accepting them\nConclusion: Evolution, Not Revolution Devin represents an evolution in AI-assisted development, not the revolution its marketing suggests. It\u0026rsquo;s a powerful tool that will likely make certain aspects of development faster and more accessible. But software engineering is about more than writing code‚Äîit\u0026rsquo;s about understanding problems, designing solutions, and building systems that serve human needs.\nThe developers who thrive will be those who learn to work with AI tools while maintaining their critical thinking, creativity, and deep technical understanding. The future isn\u0026rsquo;t AI replacing developers; it\u0026rsquo;s developers leveraging AI to build things we couldn\u0026rsquo;t imagine before.\nAs these tools evolve rapidly, this analysis is based on information available as of March 2024. The landscape will undoubtedly change, but the fundamental questions about human-AI collaboration in software development will remain relevant.\n","date":"14 March 2024","permalink":"/blog/devin/","section":"Blog","summary":"When Cognition Labs announced Devin as the \u0026ldquo;world\u0026rsquo;s first AI software engineer,\u0026rdquo; the tech community\u0026rsquo;s reaction ranged from excitement to existential dread.","title":"Thoughts on Devin: AI Software Engineer"},{"content":"A comprehensive suite of AI agents for the Pac-Man game, implementing fundamental artificial intelligence techniques from graph search to reinforcement learning. Built as part of UC Berkeley\u0026rsquo;s CS188 Introduction to Artificial Intelligence course, this project demonstrates practical applications of AI algorithms in a challenging game environment.\nThe Problem Real-world AI problems require agents that can navigate complex environments, make decisions under uncertainty, and learn from experience. While Pac-Man may seem like just a game, it presents all these challenges: path planning through mazes, adversarial ghost agents with stochastic behavior, partial observability with invisible ghosts, and the need to learn optimal strategies through trial and error.\nKey Features üîç Intelligent Path Finding\nImplemented four fundamental search algorithms (DFS, BFS, UCS, A*) to navigate mazes efficiently. Developed custom heuristics for A* that reduce node expansions by up to 80% while maintaining optimality. Solved complex problems like finding optimal paths to eat all food pellets using advanced state space representations.\nüéÆ Multi-Agent Game Playing\nBuilt adversarial search agents using Minimax and Expectimax algorithms to play against multiple ghosts. Designed sophisticated evaluation functions that balance food collection, ghost avoidance, and winning strategies. Handles both deterministic and probabilistic ghost behaviors.\nüß† Reinforcement Learning\nImplemented model-based (Value Iteration) and model-free (Q-Learning, Approximate Q-Learning) algorithms. Trained agents that learn optimal policies through experience, starting from zero knowledge about the game. Applied function approximation to handle large state spaces efficiently.\nüëª Probabilistic Tracking\nCreated ghost-hunting agents that track invisible ghosts using noisy distance sensors. Implemented Bayes Nets for probabilistic inference and particle filters for Hidden Markov Models. Achieved accurate ghost localization even with Manhattan distance readings corrupted by noise.\nüó∫Ô∏è Logic-Based Planning\nApplied propositional logic to solve SLAM (Simultaneous Localization and Mapping) problems. Built agents that can locate themselves with unknown starting positions, map unknown environments, and plan action sequences using logical inference.\nImplementation Details Search Algorithms\nThe search module implements graph-based versions of all algorithms to avoid revisiting states. Used Python\u0026rsquo;s data structures strategically‚ÄîStack for DFS, Queue for BFS, and PriorityQueue for UCS and A*. Custom heuristics for A* use Manhattan distance for simple cases and actual maze distances for complex multi-goal problems.\nGame Playing Agents\nMinimax with alpha-beta pruning handles deterministic ghosts, searching up to depth 4 in reasonable time. Expectimax models probabilistic ghost behavior, computing expected utilities across all possible ghost actions. Evaluation functions combine weighted features: food proximity, ghost distances, power pellet opportunities, and winning conditions.\nReinforcement Learning\nValue Iteration computes optimal policies for known MDPs using dynamic programming. Q-Learning agents explore using epsilon-greedy strategies, learning state-action values through experience. Approximate Q-Learning uses feature extraction to generalize across similar states, enabling learning in large state spaces.\nProbabilistic Inference\nExact inference using the Forward Algorithm for HMMs when tracking single ghosts. Particle filtering approximates joint distributions when tracking multiple ghosts simultaneously. Dynamic particle count adjustment based on uncertainty levels improves both accuracy and performance.\nKey Algorithms Search \u0026amp; Planning:\nDFS/BFS ‚Äì Complete maze exploration and shortest path finding UCS ‚Äì Optimal paths with non-uniform costs A* ‚Äì Optimal paths with admissible heuristics reducing search space Adversarial Search:\nMinimax ‚Äì Perfect play against optimal opponents Alpha-Beta Pruning ‚Äì Efficient minimax through branch elimination Expectimax ‚Äì Rational decisions against stochastic opponents Learning \u0026amp; Inference:\nValue/Policy Iteration ‚Äì Computing optimal policies for MDPs Q-Learning ‚Äì Model-free reinforcement learning Particle Filtering ‚Äì Approximate inference in continuous spaces Challenges \u0026amp; Solutions Challenge: Designing admissible heuristics for multi-goal search problems (eating all food)\nSolution: Used minimum spanning tree of remaining food as heuristic, ensuring admissibility while providing tight bounds that dramatically reduce search space\nChallenge: Balancing exploration vs exploitation in Q-Learning\nSolution: Implemented adaptive epsilon-greedy strategy that decreases exploration over time, transitioning from learning to optimal play\nChallenge: Tracking multiple invisible ghosts with noisy sensors\nSolution: Particle filter with intelligent resampling‚Äîconcentrates particles in high-probability regions while maintaining diversity to avoid particle depletion\nNote: Source code available upon request as per UC Berkeley\u0026rsquo;s academic integrity policies.\nRequest Source Code\n","date":"1 May 2022","permalink":"/projects/pacmanai/","section":"Projects","summary":"A comprehensive suite of AI agents for the Pac-Man game, implementing fundamental artificial intelligence techniques from graph search to reinforcement learning.","title":"PacmanAI"},{"content":"A lightweight version control system that implements Git\u0026rsquo;s core functionality from scratch in Java. Built as part of UC Berkeley\u0026rsquo;s CS61B Data Structures course, Gitlet demonstrates practical application of data structures including trees, hash maps, and directed acyclic graphs to solve real-world problems.\nThe Problem Understanding how version control systems work under the hood is crucial for software engineers, yet most developers only interact with Git at a surface level. This project required building a functional VCS from the ground up, handling everything from object serialization to complex merge algorithms‚Äîall while maintaining efficiency and correctness.\nKey Features üìÅ Complete Version Control\nImplements fundamental Git operations: init, add, commit, rm, status, log, and checkout. Each commit creates an immutable snapshot of the repository state, stored efficiently using SHA-1 hashing for content-addressable storage.\nüå≥ Branching \u0026amp; Merging\nFull support for creating, switching, and deleting branches. Implements a sophisticated merge algorithm that handles split points, conflict detection, and automatic merging when possible. The commit tree structure uses directed acyclic graphs to maintain branch relationships.\n‚ö° Efficient Storage\nUses blob objects for file storage with content-based deduplication‚Äîidentical files share the same blob regardless of commit or filename. Implements lazy loading and caching strategies to minimize disk I/O operations.\nüîç History Tracking\nComprehensive logging with log (current branch history) and global-log (all commits). The find command searches commit messages across the entire repository history. Status command provides detailed staging area and working directory information.\nTechnical Implementation Persistence Layer\nBuilt a custom serialization system using Java\u0026rsquo;s Serializable interface to persist repository state to disk. The .gitlet directory structure mirrors Git\u0026rsquo;s design with separate folders for commits, blobs, and branches. Each object is stored with its SHA-1 hash as the filename, enabling O(1) lookups.\nData Structures\nCommit Tree: Directed acyclic graph implementation tracking parent-child relationships Staging Area: HashMap-based structure for tracking additions and removals Blob Storage: Content-addressable storage using SHA-1 hashing for deduplication Branch Management: Reference system pointing to commit SHAs Merge Algorithm\nImplements a three-way merge that:\nFinds the split point (latest common ancestor) using BFS traversal Compares files across current branch, given branch, and split point Automatically merges non-conflicting changes Marks conflicts for manual resolution when both branches modify the same file Key Commands Repository Management:\ninit ‚Äì Creates new repository with initial commit add/rm ‚Äì Stages files for addition or removal commit ‚Äì Creates immutable snapshot with parent reference History \u0026amp; Branches:\nlog/global-log ‚Äì Views commit history branch/rm-branch ‚Äì Creates or deletes branches checkout ‚Äì Restores files or switches branches reset ‚Äì Moves current branch to specified commit Advanced Operations:\nmerge ‚Äì Three-way merge with conflict detection find ‚Äì Searches commits by message status ‚Äì Displays repository state Challenges \u0026amp; Solutions Challenge: Implementing efficient file deduplication across commits\nSolution: Content-based addressing using SHA-1 hashes‚Äîidentical files reference the same blob object regardless of name or location\nChallenge: Finding the merge split point in complex branch histories\nSolution: BFS traversal of the commit DAG to find the latest common ancestor, handling multiple merge commits correctly\nChallenge: Managing persistence without a database\nSolution: Custom serialization layer with careful file organization, using SHA-1 hashes as natural indexes for O(1) retrieval\nNote: Source code available upon request as per UC Berkeley\u0026rsquo;s academic integrity policies.\nRequest Source Code\n","date":"1 December 2021","permalink":"/projects/gitlet/","section":"Projects","summary":"A lightweight version control system that implements Git\u0026rsquo;s core functionality from scratch in Java.","title":"Gitlet"},{"content":"I\u0026rsquo;m a software engineer passionate about building scalable systems and solving complex problems. Currently studying Computer Science at UC Berkeley with hands-on experience at major tech companies‚Äîif you\u0026rsquo;d like to collaborate or discuss opportunities, feel free to reach out via LinkedIn.\nHi, I\u0026rsquo;m Christopher üëã üìç Location: San Francisco, CA\nüíª Focus: Full-Stack Development \u0026amp; Cloud Architecture\nüéì University: UC Berkeley (Class of 2026) - Computer Science, 3.79 GPA\nüìß Email: leechristopher722@gmail.com\nüîó GitHub: @leechristopher722\nMy Journey üöÄ I\u0026rsquo;m currently in my final year at UC Berkeley, where I\u0026rsquo;ve developed a strong foundation in algorithms, data structures, and system design. My academic journey has been enriched by real-world experience through competitive internships and leadership roles.\nRecent Experience:\nAmazon Web Services - Built international marketplace automation for enterprise EC2 pricing, reducing operational load by 20% and eliminating high-severity incidents OpenGrant - Engineered AI evaluation pipelines using advanced prompting techniques and created comprehensive UI/UX designs for grant proposal platforms U.S. Army (KATUSA) - Led cross-cultural teams of 50+ members, coordinating large-scale operations and serving as a linguistic liaison üìÑ View Full Resume | üõ†Ô∏è Explore My Projects\nBeyond Code üåü When I\u0026rsquo;m not coding, you\u0026rsquo;ll find me behind the bar mixing up creative cocktails for my friends‚ÄîI love experimenting with different spirits and flavors to come up with something completely new. I\u0026rsquo;m also constantly exploring San Francisco\u0026rsquo;s food scene, from hole-in-the-wall spots to trendy new restaurants, and I\u0026rsquo;m always on the mission to find the city\u0026rsquo;s most amazing coffee shops.\nWhether it\u0026rsquo;s perfecting a new cocktail recipe or discovering an amazing restaurant together, I love bringing people together over great food and drinks. These moments of connection and creativity keep me energized and inspired, both in life and in my work.\n","date":null,"permalink":"/about/","section":"Christopher Lee","summary":"I\u0026rsquo;m a software engineer passionate about building scalable systems and solving complex problems.","title":"About"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"}]